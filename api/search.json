[{"id":"f2bb7a23572ffaf10b751ed9095c199d","title":"Grafana 图表增加 Annotation 标记发布信息","content":"目的 对于已经绘制好的 grafana 的图表,我们在看到一些指标突然异常的时候,往往希望知道这个异常时间点到底发生了什么才导致指标异常的.这个时候排查可能会查看当时是否有功能上线发布. grafana 可以对图表增加 annotation 标记,天下一些有用的信息记录这个时间点发生的变化.可方便后续进行排查.\n 如图:\n\n\n如何添加grafana 可以通过接口进行添加 annotation,这对于自动化构建阶段可以很好的自动添加到 grafana 的面板.\n文档如下\nannotation  文档\n1234567891011121314POST /api/annotations HTTP/1.1Accept: application/jsonContent-Type: application/json&#123;  &quot;dashboardUID&quot;:&quot;jcIIG-07z&quot;,  &quot;panelId&quot;:1,  &quot;time&quot;:1507037197339,  &quot;tags&quot;:[&quot;tag1&quot;,&quot;tag2&quot;],  &quot;text&quot;:&quot;Annotation Description&quot;&#125;# 其中 dashboardUID 可以不写,不写的话会在所有的图表上增加创建的`annotation`# panelId 也可以不写,填写会只在这个面板上渲染,其他的不会渲染.\n\n\n创建 service account由于调用后台接口需要认证,需要先创建一个 service account 使用生成的 token 调用后台接口进行创建 annotation.\n创建步骤如下:\n\n\n\n\n保存好上面创建的 token,后续调用接口需要使用.\ngitlab cicd 自动创建 Annotation1234567891011121314151617curl -vvv --location &#x27;http://grafana.yourdomain.cn/api/annotations&#x27; \\    --header &quot;Authorization: Bearer $GRAFANA_TOKEN&quot; \\    --header &#x27;Content-Type: application/json&#x27; \\    --data &quot;&#123;        \\&quot;dashboardUID\\&quot;:\\&quot;$DASHBOARD_ID\\&quot;,        \\&quot;tags\\&quot;:[            \\&quot;namespace:$CI_PROJECT_NAMESPACE\\&quot;,            \\&quot;service:$CI_PROJECT_NAME\\&quot;,            \\&quot;author:$CI_COMMIT_AUTHOR\\&quot;,            \\&quot;commit:$CI_COMMIT_TITLE\\&quot;,            \\&quot;ref:$CI_COMMIT_REF_NAME\\&quot;,            \\&quot;job:$CI_JOB_ID\\&quot;,            \\&quot;env:$ENV\\&quot;        ],        \\&quot;text\\&quot;:\\&quot;$CI_COMMIT_AUTHOR: $CI_COMMIT_TITLE\\&quot;    &#125;&quot;\n\n以上是使用 curl 调用接口自动创建anotation 的请求.其中 GRAFANA_TOKEN 是之前创建 service account 的 token.\n该值是通过 gitlab 的 CI/CD 的 Variables 进行管理的.对于一些敏感的密钥 token 之类的可以通过该功能隐藏,不以明文的形式方式存储在代码里.\n\n同时 gitlab 在运行时也会把相关数据存储在运行时的变量里,我们可以获取到例如: 提交代码的 commit信息,提交的内容和作者等等.\n我们可以通过如下地址\nCI&#x2F;CD 预定义变量\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273...export CI_SERVER_TLS_CA_FILE=&quot;/builds/gitlab-examples/ci-debug-trace.tmp/CI_SERVER_TLS_CA_FILE&quot;if [[ -d &quot;/builds/gitlab-examples/ci-debug-trace/.git&quot; ]]; then  echo $&#x27;\\&#x27;&#x27;\\x1b[32;1mFetching changes...\\x1b[0;m&#x27;\\&#x27;&#x27;  $&#x27;\\&#x27;&#x27;cd&#x27;\\&#x27;&#x27; &quot;/builds/gitlab-examples/ci-debug-trace&quot;  $&#x27;\\&#x27;&#x27;git&#x27;\\&#x27;&#x27; &quot;config&quot; &quot;fetch.recurseSubmodules&quot; &quot;false&quot;  $&#x27;\\&#x27;&#x27;rm&#x27;\\&#x27;&#x27; &quot;-f&quot; &quot;.git/index.lock&quot;  $&#x27;\\&#x27;&#x27;git&#x27;\\&#x27;&#x27; &quot;clean&quot; &quot;-ffdx&quot;  $&#x27;\\&#x27;&#x27;git&#x27;\\&#x27;&#x27; &quot;reset&quot; &quot;--hard&quot;  $&#x27;\\&#x27;&#x27;git&#x27;\\&#x27;&#x27; &quot;remote&quot; &quot;set-url&quot; &quot;origin&quot; &quot;https://gitlab-ci-token:xxxxxxxxxxxxxxxxxxxx@example.com/gitlab-examples/ci-debug-trace.git&quot;  $&#x27;\\&#x27;&#x27;git&#x27;\\&#x27;&#x27; &quot;fetch&quot; &quot;origin&quot; &quot;--prune&quot; &quot;+refs/heads/*:refs/remotes/origin/*&quot; &quot;+refs/tags/*:refs/tags/lds&quot;++ CI_BUILDS_DIR=/builds++ export CI_PROJECT_DIR=/builds/gitlab-examples/ci-debug-trace++ CI_PROJECT_DIR=/builds/gitlab-examples/ci-debug-trace++ export CI_CONCURRENT_ID=87++ CI_CONCURRENT_ID=87++ export CI_CONCURRENT_PROJECT_ID=0++ CI_CONCURRENT_PROJECT_ID=0++ export CI_SERVER=yes++ CI_SERVER=yes++ mkdir -p /builds/gitlab-examples/ci-debug-trace.tmp++ echo -n &#x27;-----BEGIN CERTIFICATE----------END CERTIFICATE-----&#x27;++ export CI_SERVER_TLS_CA_FILE=/builds/gitlab-examples/ci-debug-trace.tmp/CI_SERVER_TLS_CA_FILE++ CI_SERVER_TLS_CA_FILE=/builds/gitlab-examples/ci-debug-trace.tmp/CI_SERVER_TLS_CA_FILE++ export CI_PIPELINE_ID=52666++ CI_PIPELINE_ID=52666++ export CI_PIPELINE_URL=https://gitlab.com/gitlab-examples/ci-debug-trace/pipelines/52666++ CI_PIPELINE_URL=https://gitlab.com/gitlab-examples/ci-debug-trace/pipelines/52666++ export CI_JOB_ID=7046507++ CI_JOB_ID=7046507++ export CI_JOB_URL=https://gitlab.com/gitlab-examples/ci-debug-trace/-/jobs/379424655++ CI_JOB_URL=https://gitlab.com/gitlab-examples/ci-debug-trace/-/jobs/379424655++ export CI_JOB_TOKEN=[MASKED]++ CI_JOB_TOKEN=[MASKED]++ export CI_REGISTRY_USER=gitlab-ci-token++ CI_REGISTRY_USER=gitlab-ci-token++ export CI_REGISTRY_PASSWORD=[MASKED]++ CI_REGISTRY_PASSWORD=[MASKED]++ export CI_REPOSITORY_URL=https://gitlab-ci-token:[MASKED]@gitlab.com/gitlab-examples/ci-debug-trace.git++ CI_REPOSITORY_URL=https://gitlab-ci-token:[MASKED]@gitlab.com/gitlab-examples/ci-debug-trace.git++ export CI_JOB_NAME=debug_trace++ CI_JOB_NAME=debug_trace++ export CI_JOB_STAGE=test++ CI_JOB_STAGE=test++ export CI_NODE_TOTAL=1++ CI_NODE_TOTAL=1++ export CI=true++ CI=true++ export GITLAB_CI=true++ GITLAB_CI=true++ export CI_SERVER_URL=https://gitlab.com:3000++ CI_SERVER_URL=https://gitlab.com:3000++ export CI_SERVER_HOST=gitlab.com++ CI_SERVER_HOST=gitlab.com++ export CI_SERVER_PORT=3000++ CI_SERVER_PORT=3000++ export CI_SERVER_SHELL_SSH_HOST=gitlab.com++ CI_SERVER_SHELL_SSH_HOST=gitlab.com++ export CI_SERVER_SHELL_SSH_PORT=22++ CI_SERVER_SHELL_SSH_PORT=22++ export CI_SERVER_PROTOCOL=https++ CI_SERVER_PROTOCOL=https++ export CI_SERVER_NAME=GitLab++ CI_SERVER_NAME=GitLab++ export GITLAB_FEATURES=audit_events,burndown_charts,code_owners,contribution_analytics,description_diffs,elastic_search,group_bulk_edit,group_burndown_charts,group_webhooks,issuable_default_templates,issue_weights,jenkins_integration,ldap_group_sync,member_lock,merge_request_approvers,multiple_issue_assignees,multiple_ldap_servers,multiple_merge_request_assignees,protected_refs_for_users,push_rules,related_issues,repository_mirrors,repository_size_limit,scoped_issue_board,usage_quotas,wip_limits,adjourned_deletion_for_projects_and_groups,admin_audit_log,auditor_user,batch_comments,blocking_merge_requests,board_assignee_lists,board_milestone_lists,ci_cd_projects,cluster_deployments,code_analytics,code_owner_approval_required,commit_committer_check,cross_project_pipelines,custom_file_templates,custom_file_templates_for_namespace,custom_project_templates,custom_prometheus_metrics,cycle_analytics_for_groups,db_load_balancing,default_project_deletion_protection,dependency_proxy,deploy_board,design_management,email_additional_text,extended_audit_events,external_authorization_service_api_management,feature_flags,file_locks,geo,github_integration,group_allowed_email_domains,group_project_templates,group_saml,issues_analytics,jira_dev_panel_integration,ldap_group_sync_filter,merge_pipelines,merge_request_performance_metrics,merge_trains,metrics_reports,multiple_approval_rules,multiple_group_issue_boards,object_storage,operations_dashboard,packages,productivity_analytics,project_aliases,protected_environments,reject_unsigned_commits,required_ci_templates,scoped_labels,service_desk,smartcard_auth,group_timelogs,type_of_work_analytics,unprotection_restrictions,ci_project_subscriptions,container_scanning,dast,dependency_scanning,epics,group_ip_restriction,incident_management,insights,license_management,personal_access_token_expiration_policy,pod_logs,prometheus_alerts,report_approver_rules,sast,security_dashboard,tracing,web_ide_terminal++ GITLAB_FEATURES=audit_events,burndown_charts,code_owners,contribution_analytics,description_diffs,elastic_search,group_bulk_edit,group_burndown_charts,group_webhooks,issuable_default_templates,issue_weights,jenkins_integration,ldap_group_sync,member_lock,merge_request_approvers,multiple_issue_assignees,multiple_ldap_servers,multiple_merge_request_assignees,protected_refs_for_users,push_rules,related_issues,repository_mirrors,repository_size_limit,scoped_issue_board,usage_quotas,wip_limits,adjourned_deletion_for_projects_and_groups,admin_audit_log,auditor_user,batch_comments,blocking_merge_requests,board_assignee_lists,board_milestone_lists,ci_cd_projects,cluster_deployments,code_analytics,code_owner_approval_required,commit_committer_check,cross_project_pipelines,custom_file_templates,custom_file_templates_for_namespace,custom_project_templates,custom_prometheus_metrics,cycle_analytics_for_groups,db_load_balancing,default_project_deletion_protection,dependency_proxy,deploy_board,design_management,email_additional_text,extended_audit_events,external_authorization_service_api_management,feature_flags,file_locks,geo,github_integration,group_allowed_email_domains,group_project_templates,group_saml,issues_analytics,jira_dev_panel_integration,ldap_group_sync_filter,merge_pipelines,merge_request_performance_metrics,merge_trains,metrics_reports,multiple_approval_rules,multiple_group_issue_boards,object_storage,operations_dashboard,packages,productivity_analytics,project_aliases,protected_environments,reject_unsigned_commits,required_ci_templates,scoped_labels,service_desk,smartcard_auth,group_timelogs,type_of_work_analytics,unprotection_restrictions,ci_project_subscriptions,cluster_health,container_scanning,dast,dependency_scanning,epics,group_ip_restriction,incident_management,insights,license_management,personal_access_token_expiration_policy,pod_logs,prometheus_alerts,report_approver_rules,sast,security_dashboard,tracing,web_ide_terminal++ export CI_PROJECT_ID=17893++ CI_PROJECT_ID=17893++ export CI_PROJECT_NAME=ci-debug-trace++ CI_PROJECT_NAME=ci-debug-trace...\n\n使用 .gitlab-ci.yml 定义发布12345678910111213141516171819202122232425stages:  - deploydeploy-to-product:  stage: deploy  environment: product  only:    - master  script:    - curl -vvv --location &#x27;http://grafana.yourdomain.cn/api/annotations&#x27; \\    --header &quot;Authorization: Bearer $GRAFANA_TOKEN&quot; \\    --header &#x27;Content-Type: application/json&#x27; \\    --data &quot;&#123;        \\&quot;dashboardUID\\&quot;:\\&quot;$DASHBOARD_ID\\&quot;,        \\&quot;tags\\&quot;:[            \\&quot;namespace:$CI_PROJECT_NAMESPACE\\&quot;,            \\&quot;service:$CI_PROJECT_NAME\\&quot;,            \\&quot;author:$CI_COMMIT_AUTHOR\\&quot;,            \\&quot;commit:$CI_COMMIT_TITLE\\&quot;,            \\&quot;ref:$CI_COMMIT_REF_NAME\\&quot;,            \\&quot;job:$CI_JOB_ID\\&quot;,            \\&quot;env:$ENV\\&quot;        ],        \\&quot;text\\&quot;:\\&quot;$CI_COMMIT_AUTHOR: $CI_COMMIT_TITLE\\&quot;    &#125;&quot;\n\n当我们合并代码到master 分支后,就会自动打上响应的annotation .这样就可以很方便的定位问题了.\n","slug":"Grafana-图表增加-Annotation-标记发布信息","date":"2024-09-23T06:26:00.000Z","categories_index":"","tags_index":"grafana,annotation,gitlab,ci/cd","author_index":"tusimo"},{"id":"b6f658d57561344b4e34417fd24455ba","title":"AWS DMS 迁移 mysql timestamp 字段时区不同步问题","content":"问题线上服务 使用dms迁移到 aws ,在迁移数据库的时候遇到 timestamp 字段迁移出现时区不同步问题.在 dms 使用fullload全量 和cdc增量模式时.fullload全量 的时区正常.cdc增量模式时区相差 8 个小时.\n调试我们源数据库在腾讯云上.mysql版本 5.7.时区为Asia/Shanghai\n123456789mysql&gt; show global variables like &#x27;%time_zone%&#x27;;+------------------+--------+| Variable_name    | Value  |+------------------+--------+| system_time_zone | CST    || time_zone        | SYSTEM |+------------------+--------+2 rows in set (0.09 sec)\n目标数据库在 aws 上,mysql版本为 5.7.时区为Asia/Shanghai\n123456789mysql&gt; show global variables like &#x27;%time_zone%&#x27;;+------------------+---------------+| Variable_name    | Value         |+------------------+---------------+| system_time_zone | UTC           || time_zone        | Asia/Shanghai |+------------------+---------------+2 rows in set (0.08 sec)\n\nDMS的 source endpoint 设置 ServerTimezone为Asia/Shanghai\n123456&#123;    &quot;Port&quot;: 63642,    &quot;ServerName&quot;: &quot;*****&quot;,    &quot;ServerTimezone&quot;: &quot;Asia/Shanghai&quot;,    &quot;Username&quot;: &quot;****&quot;&#125;\nDMS的target endpoint 没做额外的时区配置.\n我们在启动dms同步时,发现在全量模式下,timestamp字段的值是正常的.全量同步完进行增量同步的时候时区就相差 8 个小时.然而数据的插入时区也是正常的.配置当时参考了这个https://aws.amazon.com/premiumsupport/knowledge-center/dms-migrate-mysql-non-utc/\n解决在尝试无果后,咨询了AWS的技术人员.得到了一个令人诧异的答案.当时就emo了.\n12with batch-optimization，you need to use serverTimezone=Asia/Shanghai at the source endpoint like what you have done before.without batch-optimization，you need to use initstmt=SET time_zone=&#x27;Asia/Shanghai in ECA at the source endpoint.\n\n\n原来在dms里面存在一个 批量插入优化的选项.当我们在source endpoint ,使用ServerTimezone这个设置的时候,必须要开启这个批量优化的选项.不开启就会出现时区问题.开启了就正常了.dms还有一个设置时区的方法.就是使用Extra connection attributes.\n如下图,两种配置的设置位置,任选一种设置,但是不同的设置要使用不同的batch-optimization配置:\n\n\n\n\n配置\nbatch-optimization配置\n结果\n\n\n\n使用ServerTimezone&#x3D;Asia&#x2F;Shanghai\n开启\n时区均正常\n\n\n使用ServerTimezone&#x3D;Asia&#x2F;Shanghai\n不开启\n全量正常,增量同步时区不正常\n\n\n使用initstmt&#x3D;SET time_zone&#x3D;’Asia&#x2F;Shanghai’\n开启\n全量正常,增量同步时区不正常\n\n\n使用initstmt&#x3D;SET time_zone&#x3D;’Asia&#x2F;Shanghai’\n不开启\n时区均正常\n\n\n由于batch-optimization默认是不开启的,所以建议配置使用initstmt&#x3D;SET time_zone&#x3D;’Asia&#x2F;Shanghai’.\n由于腾讯云的数据库里面没有Asia/Shanghai这个时区的配置,我这里使用的是initstmt&#x3D;SET time_zone&#x3D;’+08:00’,效果是一样的.\n不要两个一起都设置.你会发现时区都不对了.\n总结这个问题困扰了很久.在得知是由于开启关闭批量插入优化导致时区不正常就非常难受.总结也不想写了.就这样吧.\n","slug":"AWS-DMS-迁移-mysql-timestamp-字段时区不同步问题","date":"2022-10-21T10:31:00.000Z","categories_index":"","tags_index":"mysql,dms,cdc,fulload,timestamp,timezone,时区","author_index":"tusimo"},{"id":"5787a32e0acc329c90bb7d6138b83d3a","title":"腾讯云 redis 迁移AWS的若干问题","content":"redis 迁移全量迁移  全量迁移比较简单,可以采用以下两种方式进行迁移.\n  1 rdb 导入导出\n2 scan keys 进行同步\n\n  以上方式可以通过 redis-shake实现.  使用比较简单,可以参考官方文档进行配置.\n增量迁移  增量迁移可以通过以下方式进行\n  1 通过PSYNC进行主从同步\n  2 通过 redis 的 pub&#x2F;sub 监听所有 keys的变化进行修改增量数据    notify-keyspace-events 可以开启 键值变化通知.通知通过 pub&#x2F;sub 输出.    该方法需要开启配置 notify-keyspace-events的值为KEA    可以使用 redis-cli工具登录控制台并使用命令config set notify-keyspace-events KEA设置.    云厂商也有后台可以修改这些配置来开启该功能.\n  psync 也可以通过 redis-shake实现增量同步.\n  但是云厂商的redis一般都是禁用了同步相关的 SYNC PSYNC 等命令.无法通过第一种方式进行同步.\n  第二种方式可以使用 riot-redis 进行同步\nriot-redis 介绍安装 - mac1brew install redis-developer/tap/riot-redis\n\n命令介绍replicatereplicate 使用 redis dump和restore命令进行同步\n参数:\n--scan-count 表示每次扫描的 key 的数量,默认值: 1000--scan-match 表示要同步的key的匹配模式,默认是*,即同步所有的 key--scan-type 表示扫描的值类型,默认值 全部类型--reader-threads 表示开启多少个读线程, 默认值: 1--reader-batch 表示单个线程 在一次pipeline里 dump的值数量 默认值:50--reader-queue 是内部共享队列的最大大小. 默认值: 10000 ,该值最小要大于--reader-threads*--reader-batch--reader-pool 表示读连接池的大小.可以小于--reader-threads.默认值:8\nreplicate-ds 使用 对应的读写命令进行还原.\n比如读取到的是 string, 那么还原的时候就使用set命令还原.\n命令示例123456789101112131415//使用默认值全量同步riot-redis -h redis.database.svc.cluster.local -p 6379 -a test replicate -h redis2.database.svc.cluster.local -p 6379 -a test//使用默认值全量同步并增量同步riot-redis -h redis.database.svc.cluster.local -p 6379 -a test replicate -h redis2.database.svc.cluster.local -p 6379 -a test --mode live//仅使用增量同步riot-redis -h redis.database.svc.cluster.local -p 6379 -a test replicate -h redis2.database.svc.cluster.local -p 6379 -a test --mode liveonly//使用自定义值全量同步并增量同步riot-redis -h redis.database.svc.cluster.local -p 6379 -a test replicate-ds -h redis2.database.svc.cluster.local -p 6379 -a test --reader-threads 4 --scan-count 2000 --reader-batch 200 --reader-queue 50000 --reader-pool 20 --mode live\n\n问题如果命令运行执行出现问题.\n可以增加 --info 或者 -d输出详细信息调试.\n123riot-redis --info -h redis.database.svc.cluster.local -p 6379 -a test replicate-ds -h redis2.database.svc.cluster.local -p 6379 -a test --reader-threads 4 --scan-count 2000 --reader-batch 200 --reader-queue 50000 --reader-pool 20 --mode liveriot-redis -d -h redis.database.svc.cluster.local -p 6379 -a test replicate-ds -h redis2.database.svc.cluster.local -p 6379 -a test --reader-threads 4 --scan-count 2000 --reader-batch 200 --reader-queue 50000 --reader-pool 20 --mode live\n\n\n连接 AWS 的 redis 报 Client set AUTH, but no password is set.  这个是由于AWS默认创建的redis没有设置密码,但是运维给了一个on ~* +@all的字符串说是密码.实际上这个是AWS的权限控制字符串.并不是密码.  所以使用空密码就不报错了.\n连接腾讯云 的 redis 报  NOAUTH Authentication required.  1 首先确认 redis 的密码是否正确  2 查看 redis 版本. 我在腾讯云上 使用 redis 4.0 版本的时候在密码正确的情况下,会报这个错误.升级到 5.0 版本就可以正常同步了.\n同步报 syntax error   可以将同步方式 修改为 replicate-ds试试看.\n","slug":"腾讯云-redis-迁移AWS的若干问题","date":"2022-08-22T07:09:00.000Z","categories_index":"redis","tags_index":"redis,腾讯云,redis-shake,riot-redis","author_index":"tusimo"},{"id":"7217aeb6d9a4e768dbd053e0d0297223","title":"高峰时期上线 HPA 引起的副本抖动","content":"现象我们在引入kubernetes的hpa功能后,高峰时期上线我们的pod副本数会缩小到replicas指定的数量后,然后hpa又会开始扩容,这个时候我们的pod无法支撑请求数,会出现大量的超时.等到hpa扩容后会恢复,导致高峰时期不敢轻易上线.\n原因没有hpa之前,我们的副本数量由replicas参数控制,假如设置replicas:2,那么我们的pod副本数就一直都是 2 个.除非我们手动进行扩容.引入hpa后,副本数会由hpa的minReplicas和maxReplicas控制.发布的时候会先平衡到replicas的值,然后hpa会根据配置动态的修改replicas值来控制pod的副本数.假如hpa后我们的副本数为:4,大于初始值replicas:2的值了.那么发布的时候就会先缩小到replicas:2的值.再扩容到发布前的副本数:4,这个时候就会出现副本数的抖动.\n演示图\n解决办法使用hpa后删除所有deployments里面设置的初始的replicas节点.不设置replicas,那么发布的时候,会根据当前的数量按照设置的更新策略进行.而不是先恢复到replicas的值.那么这个时候初始的副本数则有minReplicas控制.\n题外话当我们使用滚动更新的时候,要保证副本数最小的可用副本数.不能把所有的副本都Terminating了.这样会引起服务异常.尤其是我们在开发测试环境的时候.由于请求量不大,往往副本数只有一个.当我们滚动更新的时候,可能会导致副本数清零,再启动新副本.\n1234567strategy:    rollingUpdate:      maxSurge: 1      maxUnavailable: 0    type: RollingUpdate\n\n\n\n\n\n\n\n通俗讲\n\nmaxSure 控制加的行为,每次新增多少个副本.可以为整数和百分比\nmaxUnavailable 控制减的行为, 表示最多不可用的数量\n\n\n\nstrategy（更新策略）：　　.spec.strategy 指定新的Pod替换旧的Pod的策略。 .spec.strategy.type 可以是Recreate或者是 RollingUpdate。RollingUpdate是默认值。\n　　Recreate： 重建式更新，就是删一个建一个。类似于ReplicaSet的更新方式，即首先删除现有的Pod对象，然后由控制器基于新模板重新创建新版本资源对象。\n　　rollingUpdate：滚动更新，简单定义 更新期间pod最多有几个等。可以指定maxUnavailable 和 maxSurge 来控制 rolling update 进程。\n　　maxSurge：.spec.strategy.rollingUpdate.maxSurge 是可选配置项，用来指定可以超过期望的Pod数量的最大个数。该值可以是一个绝对值（例如5）或者是期望的Pod数量的百分比（例如10%）。当        MaxUnavailable为0时该值不可以为0。通过百分比计算的绝对值向上取整。默认值是1。\n　　例如，该值设置成30%，启动rolling update后新的ReplicatSet将会立即扩容，新老Pod的总数不能超过期望的Pod数量的130%。旧的Pod被杀掉后，新的ReplicaSet将继续扩容，旧的ReplicaSet会进一步缩容，确保在升级的所有时刻所有的Pod数量和不会超过期望Pod数量的130%。\n　　maxUnavailable：.spec.strategy.rollingUpdate.maxUnavailable 是可选配置项，用来指定在升级过程中不可用Pod的最大数量。该值可以是一个绝对值（例如5），也可以是期望Pod数量的百分比（例如10%）。通过计算百分比的绝对值向下取整。  如果.spec.strategy.rollingUpdate.maxSurge 为0时，这个值不可以为0。默认值是1。\n　　例如，该值设置成30%，启动rolling update后旧的ReplicatSet将会立即缩容到期望的Pod数量的70%。新的Pod ready后，随着新的ReplicaSet的扩容，旧的ReplicaSet会进一步缩容确保在升级的所有时刻可以用的Pod数量至少是期望Pod数量的70%。\nPS：maxSurge和maxUnavailable的属性值不可同时为0，否则Pod对象的副本数量在符合用户期望的数量后无法做出合理变动以进行更新操作。\n　　在配置时，用户还可以使用Deployment控制器的spec.minReadySeconds属性来控制应用升级的速度。新旧更替过程中，新创建的Pod对象一旦成功响应就绪探测即被认为是可用状态，然后进行下一轮的替换。而spec.minReadySeconds能够定义在新的Pod对象创建后至少需要等待多长的时间才能会被认为其就绪，在该段时间内，更新操作会被阻塞。\n\n\n\n\n\n\n给个稳妥的配置,开发测试环境妥妥的\n\nmaxSure:1 一个一个滚动更新,就是发布慢了点.\nmaxUnavailable:0 至少有一个副本在运行\n\n\n\n","slug":"高峰时期上线-HPA-引起的副本抖动","date":"2022-08-11T13:11:00.000Z","categories_index":"kubernetes","tags_index":"kubernetes,hpa,pod","author_index":"tusimo"},{"id":"b14c7bb0c256981c822f52d59e144945","title":"php 容器化的几点建议","content":"镜像编译基础镜像编译github repo\n基础镜像编译使用环境变量替换配置.\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138FROM php:7.2-fpm-alpine# replace repositoriesRUN sed -i &#x27;s/dl-cdn.alpinelinux.org/mirrors.cloud.tencent.com/g&#x27; /etc/apk/repositories# add timezoneRUN apk add -u --no-cache tzdata \\ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN apk upgrade &amp;&amp; \\    apk add --no-cache curl \\        boost-dev \\        git \\        ca-certificates \\        automake \\        libtool \\        file \\        linux-headers \\        re2c \\        pkgconf \\        openssl-dev \\        curl-dev \\        autoconf \\        openssl \\        gcc \\        make \\        g++ \\        zlib-dev \\        graphviz \\        libpng-dev \\        libpq \\        icu-dev \\        libffi-dev \\        freetype-dev \\        libxslt-dev \\        libjpeg-turbo-dev \\        libwebp-dev \\        libmemcached-dev \\        libmcrypt-dev \\        libzip-dev \\        librdkafka-dev &amp;&amp; \\    docker-php-ext-configure gd \\      --with-gd \\      --with-freetype-dir=/usr/include/ \\      --with-png-dir=/usr/include/ \\      --with-jpeg-dir=/usr/include/ \\      --with-webp-dir=/usr/include/ &amp;&amp; \\    docker-php-ext-install fileinfo pdo_mysql mysqli gd exif intl xsl soap zip opcache sockets bcmath pcntl &amp;&amp; \\    docker-php-source deleteRUN pecl install redis-5.0.2 memcached-3.1.4 rdkafka yaf-3.0.8 yar-2.0.5 mcrypt hprose-1.6.8 \\    &amp;&amp; docker-php-ext-enable redis memcached rdkafka yaf yar mcrypt hproseRUN wget https://storage.googleapis.com/downloads.webmproject.org/releases/webp/libwebp-1.1.0.tar.gz -O /tmp/libwebp-1.1.0.tar.gz \\    &amp;&amp; tar -C /tmp -zxvf /tmp/libwebp-1.1.0.tar.gz \\    &amp;&amp; cd /tmp/libwebp-1.1.0 \\    &amp;&amp; ./configure --prefix=/usr/local/libwebp --enable-everything \\    &amp;&amp; make &amp;&amp; make installRUN apk del autoconf gcc make g++ \\    &amp;&amp; rm -fr /var/cache/apk/* /tmp/* /usr/share/manWORKDIR /var/www/html# use env control php and php-fpmENV TZ=Asia/ShanghaiENV APP_ENV=productENV PHP_DATE_TIMEZONE=&quot;Asia/Shanghai&quot;ENV PHP_ERROR_LOG=&quot;/proc/self/fd/2&quot;ENV PHP_LOG_LEVEL=&quot;notice&quot;ENV PHP_PROCESS_MAX=0ENV PHP_RLIMIT_FILES=51200ENV PHP_RLIMIT_CORE=0ENV PHP_USER=www-dataENV PHP_GROUP=www-dataENV PHP_LISTEN=0.0.0.0:9000ENV PHP_PM=staticENV PHP_PM_MAX_CHILDREN=20ENV PHP_PM_START_SERVERS=4ENV PHP_PM_MIN_SPARE_SERVERS=2ENV PHP_PM_MAX_SPARE_SERVERS=10ENV PHP_PM_PROCESS_IDLE_TIMEOUT=10sENV PHP_PM_MAX_REQUESTS=10000ENV PHP_SLOWLOG=&quot;/proc/self/fd/2&quot;ENV PHP_REQUEST_SLOWLOG_TIMEOUT=&quot;2s&quot;ENV PHP_REQUEST_TERMINATE_TIMEOUT=&quot;120s&quot;ENV PHP_MAX_EXECUTION_TIME=600ENV PHP_MAX_INPUT_TIME=60ENV PHP_MEMORY_LIMIT=384MENV PHP_ERROR_REPORTING=&quot;E_ALL &amp; ~E_DEPRECATED &amp; ~E_STRICT&quot;ENV PHP_DISPLAY_ERRORS=&quot;Off&quot;ENV PHP_DISPLAY_STARTUP_ERRORS=&quot;Off&quot;ENV PHP_POST_MAX_SIZE=100MENV PHP_UPLOAD_MAX_FILESIZE=50MENV PHP_MAX_FILE_UPLOADS=20ENV PHP_ACCESS_LOG=&quot;/dev/null&quot;ENV PHP_TRACK_ERRORS=OffENV PHP_ACCESS_FORMAT=&quot;&#123; \\&quot;type\\&quot;: \\&quot;access\\&quot;, \\&quot;time\\&quot;: \\&quot;%t\\&quot;, \\&quot;environment\\&quot;: \\&quot;%&#123;APP_ENV&#125;e\\&quot;, \\&quot;method\\&quot;: \\&quot;%m\\&quot;, \\&quot;request_uri\\&quot;: \\&quot;%r%Q%q\\&quot;, \\&quot;status_code\\&quot;: \\&quot;%s\\&quot;, \\&quot;cost_time\\&quot;: %&#123;mili&#125;d, \\&quot;cpu_usage\\&quot;: &#123; \\&quot;user\\&quot; : %&#123;user&#125;C, \\&quot;system\\&quot;: %&#123;system&#125;C, \\&quot;total\\&quot;: %&#123;total&#125;C &#125;, \\&quot;memory_usage\\&quot;: %&#123;bytes&#125;M, \\&quot;remote_ip\\&quot;: \\&quot;%R\\&quot;, \\&quot;module\\&quot;: \\&quot;php-fpm\\&quot;, \\&quot;log_type\\&quot;: \\&quot;access-log\\&quot; &#125;&quot;ENV PHP_YAF_USE_NAMESPACE=OffENV PHP_YAF_USE_SPL_AUTOLOAD=OnENV PHP_YAR_CONNECT_TIMEOUT=1000ENV PHP_YAR_TIMEOUT=5000ENV PHP_YAR_DEBUG=offENV PHP_OPCACHE_ENABLE=1ENV PHP_OPCACHE_ENABLE_CLI=1ENV PHP_OPCACHE_MEMORY_CONSUMPTION=128ENV PHP_OPCACHE_INTERNED_STRINGS_BUFFER=8ENV PHP_OPCACHE_MAX_ACCELERATED_FILES=100000ENV PHP_OPCACHE_MAX_WASTED_PERCENTAGE=5ENV PHP_OPCACHE_USE_CWD=1ENV PHP_OPCACHE_VALIDATE_TIMESTAMPS=0ENV PHP_OPCACHE_REVALIDATE_FREQ=0ENV PHP_OPCACHE_FAST_SHUTDOWN=1ENV PHP_OPCACHE_CONSISTENCY_CHECKS=0ENV PHP_OPCACHE_BLACKLIST_FILENAME=/var/www/html/.opcacheignoreCOPY php-config/php.ini &quot;$PHP_INI_DIR&quot;COPY php-config/conf.d/ &quot;$PHP_INI_DIR&quot;/conf.d/COPY php-config/php-fpm.conf /usr/local/etc/COPY php-config/www.conf /usr/local/etc/php-fpm.d/EXPOSE 9000RUN rm -fr /usr/local/etc/php-fpm.d/zz-docker.confCOPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]CMD [&quot;php-fpm&quot;]\n\n通过环境变量控制程序的行为.替换 php 的配置为 $&#123;XXX&#125;, 然后在Dockerfile中设置该环境变量.后续容器在运行时,可以通过改变环境变量来修改运行时的配置.\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135;;;;;;;;;;;;;;;;;;;;;; FPM Configuration ;;;;;;;;;;;;;;;;;;;;;;; All relative paths in this configuration file are relative to PHP&#x27;s install; prefix (/usr/local). This prefix can be dynamically changed by using the; &#x27;-p&#x27; argument from the command line.;;;;;;;;;;;;;;;;;;; Global Options ;;;;;;;;;;;;;;;;;;;[global]; Pid file; Note: the default prefix is /usr/local/var; Default Value: none;pid = run/php-fpm.pid; Error log file; If it&#x27;s set to &quot;syslog&quot;, log is sent to syslogd instead of being written; into a local file.; Note: the default prefix is /usr/local/var; Default Value: log/php-fpm.logerror_log = $&#123;PHP_ERROR_LOG&#125;; syslog_facility is used to specify what type of program is logging the; message. This lets syslogd specify that messages from different facilities; will be handled differently.; See syslog(3) for possible values (ex daemon equiv LOG_DAEMON); Default Value: daemon;syslog.facility = daemon; syslog_ident is prepended to every message. If you have multiple FPM; instances running on the same server, you can change the default value; which must suit common needs.; Default Value: php-fpm;syslog.ident = php-fpm; Log level; Possible Values: alert, error, warning, notice, debug; Default Value: noticelog_level = $&#123;PHP_LOG_LEVEL&#125;; Log buffering specifies if the log line is buffered which means that the; line is written in a single write operation. If the value is false, then the; data is written directly into the file descriptor. It is an experimental; option that can potentionaly improve logging performance and memory usage; for some heavy logging scenarios. This option is ignored if logging to syslog; as it has to be always buffered.; Default value: yes;log_buffering = no; If this number of child processes exit with SIGSEGV or SIGBUS within the time; interval set by emergency_restart_interval then FPM will restart. A value; of &#x27;0&#x27; means &#x27;Off&#x27;.; Default Value: 0;emergency_restart_threshold = 0; Interval of time used by emergency_restart_interval to determine when; a graceful restart will be initiated.  This can be useful to work around; accidental corruptions in an accelerator&#x27;s shared memory.; Available Units: s(econds), m(inutes), h(ours), or d(ays); Default Unit: seconds; Default Value: 0;emergency_restart_interval = 0; Time limit for child processes to wait for a reaction on signals from master.; Available units: s(econds), m(inutes), h(ours), or d(ays); Default Unit: seconds; Default Value: 0;process_control_timeout = 0; The maximum number of processes FPM will fork. This has been designed to control; the global number of processes when using dynamic PM within a lot of pools.; Use it with caution.; Note: A value of 0 indicates no limit; Default Value: 0process.max = $&#123;PHP_PROCESS_MAX&#125;; Specify the nice(2) priority to apply to the master process (only if set); The value can vary from -19 (highest priority) to 20 (lowest priority); Note: - It will only work if the FPM master process is launched as root;       - The pool process will inherit the master process priority;         unless specified otherwise; Default Value: no set; process.priority = -19; Send FPM to background. Set to &#x27;no&#x27; to keep FPM in foreground for debugging.; Default Value: yesdaemonize = no; Set open file descriptor rlimit for the master process.; Default Value: system defined valuerlimit_files = $&#123;PHP_RLIMIT_FILES&#125;; Set max core size rlimit for the master process.; Possible Values: &#x27;unlimited&#x27; or an integer greater or equal to 0; Default Value: system defined value;rlimit_core = 0; Specify the event mechanism FPM will use. The following is available:; - select     (any POSIX os); - poll       (any POSIX os); - epoll      (linux &gt;= 2.5.44); - kqueue     (FreeBSD &gt;= 4.1, OpenBSD &gt;= 2.9, NetBSD &gt;= 2.0); - /dev/poll  (Solaris &gt;= 7); - port       (Solaris &gt;= 10); Default Value: not set (auto detection)events.mechanism = epoll; When FPM is built with systemd integration, specify the interval,; in seconds, between health report notification to systemd.; Set to 0 to disable.; Available Units: s(econds), m(inutes), h(ours); Default Unit: seconds; Default value: 10;systemd_interval = 10;;;;;;;;;;;;;;;;;;;;; Pool Definitions ;;;;;;;;;;;;;;;;;;;;;; Multiple pools of child processes may be started with different listening; ports and different management options.  The name of the pool will be; used in logs and stats. There is no limitation on the number of pools which; FPM can handle. Your system will tell you anyway :); Include one or more files. If glob(3) exists, it is used to include a bunch of; files from a glob(3) pattern. This directive can be used everywhere in the; file.; Relative path can also be used. They will be prefixed by:;  - the global prefix if it&#x27;s been set (-p argument);  - /usr/local otherwiseinclude=etc/php-fpm.d/*.conf\n\n服务镜像编译往往 php-fpm 一般和 nginx配合来提供服务,  由于 php-fpm 与 nginx 是通过 cgi 通信, 所以在编译镜像的时候就要考虑nginx 和 php-fpm是如何通信和部署的.在传统的方式下,一般通过在宿主机部署nginx 和 php-fpm两个程序并通过 cgi进行通信.\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859server &#123;    server_name _;    listen      80 default_server;    root        /var/www/html;    index  index.php index.html index.htm;\tadd_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;*&#x27;;\tadd_header &#x27;Access-Control-Allow-Headers&#x27; &#x27;*&#x27;;\tadd_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;*&#x27;;\tif ($request_method = &#x27;OPTIONS&#x27;) &#123;\t\t\treturn 204;\t&#125;\tlocation /nginx-status &#123;        stub_status;        access_log off;        allow 127.0.0.1;        deny all;    &#125;\tlocation / &#123;\t\ttry_files $uri $uri/ /index.php$is_args$args;\t&#125;\tlocation = /favicon.ico &#123; access_log off; log_not_found off; &#125;    location = /robots.txt  &#123; access_log off; log_not_found off; &#125;\tlocation ~ \\.(jpg|jpeg|gif|png|css|js|ico|xml|swf)$ &#123;\t\tetag           on;\t\texpires        max;\t\taccess_log     off;\t\tlog_not_found  off;\t&#125;\t\tlocation ~ [^/]\\.php(/|$) &#123;\t\ttry_files $uri =404;                      \t\tinclude        fastcgi_params;                                     \t\tfastcgi_index index.php;                                           \t\tfastcgi_split_path_info ^(.+\\.php)(/.+)$;                          \t\tfastcgi_pass   unix:/dev/shm/php-cgi.sock;                         \t\tfastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;\t&#125;\tlocation ~ /\\.(?!well-known).* &#123;        deny all;    &#125;\t# return 404 for all other php files not matching the front controller    # this prevents access to other php files you don&#x27;t want to be accessible.    location ~ \\.php$ &#123;\t\treturn 404;    &#125;&#125;\n\n以上是一个通过 unix socket进行通信的方式,该配置对应 php-fpm 的配置为: listen: /dev/shm/php-cgi.sock. 该方式需要nginx和 php-fpm在同一个机器上.\n以上是一个通过 unix socket进行通信的方式,该配置对应 php-fpm 的配置为: listen: /dev/shm/php-cgi.sock. 该方式需要nginx和 php-fpm在同一个机器上.\n我们也可以使用网络进行通信.设置 fastcgi_pass 为 127.0.0.1:9000 以及 php-fpm 的 listen:0.0.0.0:9000.通过网络的方式不需要nginx和 php-fpm在同一个机器上.\nphp业务代码编译\n\n\n\n\n\nTIP\n\n使用分阶段构建,编译和运行分开,减少不必要的资源引入\ncomposer只在编译阶段引入,运行时无需开启\n基础 php-fpm使用小体积镜像\n\n\n\n123456789101112131415161718192021222324252627ARG BASE_TAG# First stage: build the executable.FROM composer AS builder# Set the working directory outside $GOPATH to enable the support for modules.WORKDIR /src# Import the code from the context.COPY ./composer.json ./composer.lock /src/RUN composer install --no-dev --prefer-dist --no-autoloaderCOPY ./ /src/RUN composer dump-autoload -o &amp;&amp; composer clear-cache# Final stage: the running container.FROM php-fpm  AS final# Import the compiled executable from the first stage.COPY --from=builder /src /var/www/htmlWORKDIR /var/www/htmlEXPOSE 9000\n\n下面的图代表了两种打包方式:\n分开打包将nginx和php-fpm分别打包成两个镜像.\n\n合并打包将nginx和php-fpm打包到同一个镜像,使用supervisor,pm2等进程管理工具管理进程.\n\n镜像部署目前流行的部署方式一般都是部署到kubernetes中,所以我们以kubernetes部署为例来分析下几种不同部署方式的优缺点.\nnginx 和 php-fpm 单独部署将nginx和php-fpm部署成两个deployments,使用service进行通信.\n\n\n\n\n\n\n\n优点\n\nnginx和php-fpm可以进行单独扩缩容\nnginx和php-fpm可以进行单独配置和更新,不相互影响\nnginx和php-fpm使用网络进行通信\n发布代码简单,只需要更新 php-fpm 的镜像即可\n比较灵活,节省资源\n\n\n\n\n\n\n\n\n\n\n缺点\n\n部署比较麻烦,需要同时暴露80,9000端口\n容器的健康检查需要单独检查,无法针对业务接口设置健康检查\n\n\n\nnginx 和 php-fpm 部署到同一个 pod部署一个deployments并使用两个 container部署到同一个pod里面.\n\n\n\n\n\n\n优点\n\n无需暴露php-fpm的端口,内部容器通过共享网络栈通信\n部署相对简单,通过pod内部署多个container进行协作完成服务工作\n\n\n\n\n\n\n\n\n\n\n缺点\n\n扩缩容需要一起,无法单独扩缩容\n相对浪费资源\n\n\n\nnginx 和 php-fpm 共同打包部署到一个 pod\n\n\n\n\n\n\n优点\n\npod内只有一个container,部署简单\n可以检查服务的健康\n\n\n\n\n\n\n\n\n\n\n缺点\n\n扩缩容需要一起,无法单独扩缩容\n浪费资源\n\n\n\n程序运行php-fpm pm 管理配置在kubernetes中一般会使用hpa根据资源的消耗进行自动扩缩容.如果选用ondemand,dynamic的方式,内存使用会存在波动,如果针对内存使用进行hpa往往来不及等到调度成功并运行新的pod进行服务.在内存充足的情况下使用static的方式固定内存使用.hpa可以只根据cpu进行 hpa会更好.\nPHP_PM_MAX_CHILDREN 并不是越大越好.我们可以开启php-fpm自带的metrics来观察 php-fpm的使用情况.\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100; The URI to view the FPM status page. If this value is not set, no URI will be; recognized as a status page. It shows the following informations:;   pool                 - the name of the pool;;   process manager      - static, dynamic or ondemand;;   start time           - the date and time FPM has started;;   start since          - number of seconds since FPM has started;;   accepted conn        - the number of request accepted by the pool;;   listen queue         - the number of request in the queue of pending;                          connections (see backlog in listen(2));;   max listen queue     - the maximum number of requests in the queue;                          of pending connections since FPM has started;;   listen queue len     - the size of the socket queue of pending connections;;   idle processes       - the number of idle processes;;   active processes     - the number of active processes;;   total processes      - the number of idle + active processes;;   max active processes - the maximum number of active processes since FPM;                          has started;;   max children reached - number of times, the process limit has been reached,;                          when pm tries to start more children (works only for;                          pm &#x27;dynamic&#x27; and &#x27;ondemand&#x27;);; Value are updated in real time.; Example output:;   pool:                 www;   process manager:      static;   start time:           01/Jul/2011:17:53:49 +0200;   start since:          62636;   accepted conn:        190460;   listen queue:         0;   max listen queue:     1;   listen queue len:     42;   idle processes:       4;   active processes:     11;   total processes:      15;   max active processes: 12;   max children reached: 0;; By default the status page output is formatted as text/plain. Passing either; &#x27;html&#x27;, &#x27;xml&#x27; or &#x27;json&#x27; in the query string will return the corresponding; output syntax. Example:;   http://www.foo.bar/status;   http://www.foo.bar/status?json;   http://www.foo.bar/status?html;   http://www.foo.bar/status?xml;; By default the status page only outputs short status. Passing &#x27;full&#x27; in the; query string will also return status for each pool process.; Example:;   http://www.foo.bar/status?full;   http://www.foo.bar/status?json&amp;full;   http://www.foo.bar/status?html&amp;full;   http://www.foo.bar/status?xml&amp;full; The Full status returns for each process:;   pid                  - the PID of the process;;   state                - the state of the process (Idle, Running, ...);;   start time           - the date and time the process has started;;   start since          - the number of seconds since the process has started;;   requests             - the number of requests the process has served;;   request duration     - the duration in µs of the requests;;   request method       - the request method (GET, POST, ...);;   request URI          - the request URI with the query string;;   content length       - the content length of the request (only with POST);;   user                 - the user (PHP_AUTH_USER) (or &#x27;-&#x27; if not set);;   script               - the main script called (or &#x27;-&#x27; if not set);;   last request cpu     - the %cpu the last request consumed;                          it&#x27;s always 0 if the process is not in Idle state;                          because CPU calculation is done when the request;                          processing has terminated;;   last request memory  - the max amount of memory the last request consumed;                          it&#x27;s always 0 if the process is not in Idle state;                          because memory calculation is done when the request;                          processing has terminated;; If the process is in Idle state, then informations are related to the; last request the process has served. Otherwise informations are related to; the current request being served.; Example output:;   ************************;   pid:                  31330;   state:                Running;   start time:           01/Jul/2011:17:53:49 +0200;   start since:          63087;   requests:             12808;   request duration:     1250261;   request method:       GET;   request URI:          /test_mem.php?N=10000;   content length:       0;   user:                 -;   script:               /home/fat/web/docs/php/test_mem.php;   last request cpu:     0.00;   last request memory:  0;; Note: There is a real-time FPM status monitoring sample web page available;       It&#x27;s available in: /usr/local/share/php/fpm/status.html;; Note: The value must start with a leading slash (/). The value can be;       anything, but it may not be a good idea to use the .php extension or it;       may conflict with a real PHP file.; Default Value: not setpm.status_path = /fpm-status\n\n以上配置开启了监控.\n使用php-fpm-exporter来暴露数据给prometheus拉取查看运行情况.\n123456# 使用 unix 通信 启动 /usr/local/bin/php-fpm-exporter server --phpfpm.scrape-uri &quot;unix:///dev/shm/php-cgi.sock;/fpm-status&quot; --phpfpm.fix-process-count true# 使用 tcp 通信 启动/usr/local/bin/php-fpm-exporter server --phpfpm.scrape-uri &quot;tcp://127.0.0.1:9000/status&quot; --phpfpm.fix-process-count true\n\n查看 Running Idle 数据.可以看到服务一般同时运行的数量,一般同时运行的数量都是比较少的.不足 10 个.\n\n如果响应比较慢就会发现同时运行的数量会攀升.部分慢接口会导致整个服务都会被拖垮.\n\n一般接口都要设置服务端的超时和客户端的超时控制.\n服务端可以通过PHP_REQUEST_TERMINATE_TIMEOUT 控制请求的执行时长.客户端可以通过设置 connect_timeout read_timeout 等限制接口的执行时长.避免慢接口堵塞请求导致服务大面积超时.\n12345678910111213141516171819202122232425262728;;输出到控制台ENV PHP_ERROR_LOG=&quot;/proc/self/fd/2&quot; ENV PHP_RLIMIT_FILES=51200ENV PHP_RLIMIT_CORE=0ENV PHP_USER=www-dataENV PHP_GROUP=www-dataENV PHP_LISTEN=0.0.0.0:9000ENV PHP_PM=staticENV PHP_PM_MAX_CHILDREN=20ENV PHP_PM_START_SERVERS=4ENV PHP_PM_MIN_SPARE_SERVERS=2ENV PHP_PM_MAX_SPARE_SERVERS=10ENV PHP_PM_PROCESS_IDLE_TIMEOUT=10s;;请求超时 10000 次重启 fpm 进程ENV PHP_PM_MAX_REQUESTS=10000ENV PHP_SLOWLOG=&quot;/proc/self/fd/2&quot;;;慢接口时长ENV PHP_REQUEST_SLOWLOG_TIMEOUT=&quot;2s&quot;;;请求最大执行时长ENV PHP_REQUEST_TERMINATE_TIMEOUT=&quot;120s&quot;;;cli 模式脚本最大执行时长ENV PHP_MAX_EXECUTION_TIME=600ENV PHP_MAX_INPUT_TIME=60;; cli内存限制ENV PHP_MEMORY_LIMIT=384MENV PHP_ERROR_REPORTING=&quot;E_ALL &amp; ~E_DEPRECATED &amp; ~E_STRICT&quot;;;不输出日志ENV PHP_ACCESS_LOG=&quot;/dev/null&quot;\n\n高峰时期接口超时1 查看php-fpm数量是否不够2 cpu是否不够3 查看 socket 连接是否不够\n之前我们系统迁移kubernetes低峰时期运行良好.高峰时期会阶段性的连接超时.经过排查,发现是 socket 连接数不够用了.我们服务大多是短连接,占用较多的连接数.大量连接数在TIME_WAIT.导致无可用socket来建立连接.出现接口大量超时.由于我们没有关注connect_timeout.排查起来非常困难.后来使用pod里面的init_container来动态修改内核参数.一般修改网络相关的参数.参考如下:\n有部分参数由于腾讯云虚拟节点的限制无法修改,所以只改了部分参数.修改完之后我们连接超时的问题就消失了.\n123456789101112#!/bin/shecho &quot;start optimize&quot;sysctl -w net.ipv4.tcp_max_syn_backlog=16384sysctl -w net.ipv4.tcp_max_tw_buckets=32768sysctl -w net.core.somaxconn=32768sysctl -w net.ipv4.ip_local_port_range=&quot;10000 61000&quot;echo &quot;optimize done&quot;\n\n关于日志收集为了便于请求日式的收集,慢接口的日志收集.我们选择将所有的日志使用json的格式输出到控制台上.\n\n\n\n\n\n\n\n\n\nphp-fpm的慢接口日志无法修改格式\nkubernetes会将所有容器的控制台日志保存在文件里.我们通过daemonsets在每个节点部署filebeat收集日志并发送到elasticsearch.\n","slug":"php-容器化的几点建议","date":"2022-08-10T10:15:00.000Z","categories_index":"kubernetes,php","tags_index":"php,kubernetes,nginx,docker","author_index":"tusimo"},{"id":"8c86b39e74b792721702587f520ee80d","title":"Istio 无法支持 HTTP1.0 和访问接口存在延迟的解决办法","content":"现象Rancher 通过 RKE 部署了 K8S 集群后,启动了 Istio,由于一些项目使用的事 HTTP/1.0协议,访问出现问题. Istio 默认只支持 HTTP/1.1 以上协议版本，并不支持 HTTP/1.0。\n原因Istio 默认不支持 HTTP1.0 协议\nIstio 配置参考\n\n解决办法修改环境变量 PILOT_HTTP10 为 1,即可增加对 HTTP1.0的支持。\nRancher 部署的ISTIO\n主要方式是修改 环境变量的值.修改Deployments 中 istiod 资源的配置.增加环境变量 PILOT_HTTP10的值为 1 即可.\n\n结果修改后 HTTP1.0已经可以支持了.但是出现了另外的问题.访问接口的时候出现了超时.所有接口访问速度都增加了大概 1 秒的样子.\n解决 1 秒问题查阅 Envoy的文档后发现,\nEnvory相关配置地址\n\n修改改配置为 0 可以解决这个 1 秒延迟的问题.\n\n创建以下 disable-close-timeout.yaml 并配置到 kubernetes 集群 : kubectl apply -f disable-close-timeout.yaml -n istio-system\n123456789101112131415161718192021apiVersion: networking.istio.io/v1alpha3kind: EnvoyFiltermetadata:  name: custom-protocol  namespace: istio-systemspec:  configPatches:  - applyTo: NETWORK_FILTER    match:      listener:        filterChain:          filter:            name: envoy.filters.network.http_connection_manager    patch:      operation: MERGE      value:        name: envoy.filters.network.http_connection_manager        typed_config:          &#x27;@type&#x27;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager          delayed_close_timeout: &quot;0&quot;","slug":"Istio无法支持-HTTP1-0-和访问接口存在延迟的解决办法","date":"2022-08-10T06:24:00.000Z","categories_index":"Kubernetes","tags_index":"RKE,Rancher,Istio,HTTP,Kubernetes,Envoy","author_index":"tusimo"},{"id":"f73a8e23e6f6f669cf99c7dba8fa0722","title":"","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server1$ hexo server\n\nMore info: Server\nGenerate static files1$ hexo generate\n\nMore info: Generating\nDeploy to remote sites1$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2022-08-08T13:35:00.000Z","categories_index":"","tags_index":"","author_index":"tusimo"}]